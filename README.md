ğŸ§ Multimodal Audio Retrieval using CLAP + FAISS + Streamlit

A Text-to-Audio Retrieval System using Contrastive Language-Audio Pretraining (CLAP) that indexes audio samples (Drums, Keys) and enables prompt-based search. The embeddings are stored in a FAISS vector database, making similarity-based retrieval fast and scalable.
ğŸ“Œ Objective

Build a system that:

âœ” Extracts audio embeddings using CLAP
âœ” Supports Text â†’ Audio and Audio â†’ Audio similarity search
âœ” Stores embeddings in FAISS Vector DB
âœ” Generates labels using Zero-Shot Text prompts
âœ” Provides a Streamlit UI for retrieval
âœ” Evaluates performance using Confusion Matrix + Classification Report
ğŸ“‚ Dataset

Source: loperman.com
Classes:

ğŸ¥ Drums

ğŸ¹ Keys

Class	Samples Count
Drums	20
Keys	20
Data/
   drums/
   keys/
metadata_simple.csv

| Model          | CLAP (laion/clap-htsat-unfused)          |
| -------------- | ---------------------------------------- |
| Embedding Type | Joint Text & Audio Embeddings            |
| Trained For    | Audio classification, retrieval, tagging |
| Frameworks     | PyTorch + Transformers                   |

     Audio Files  â”€â”€â–º CLAP Audio Encoder â”€â”€â–º Embeddings â”
                                                         â”‚
                                                         â–¼
            Text Query â”€â–º CLAP Text Encoder â”€â–º Vector Similarity â”€â–º Top-K Audio Results
                                                         â”‚
                                                         â–¼
                                                  FAISS Vector DB

| Feature                          | Status |
| -------------------------------- | ------ |
| Audio Embeddings + FAISS Index   | âœ…      |
| Streamlit UI for Retrieval       | âœ…      |
| Zero-shot Labeling using CLAP    | âœ…      |
| Audioâ†’Audio Similarity Search    | âœ…      |
| Evaluation with Confusion Matrix | âœ…      |
| Save CM + Report as PNG/TXT      | âœ…      |

git clone <your_repo_url>
cd multimodal-clap-faiss
pip install -r requirements.txt

â–¶ï¸ Usage
ğŸ— Build embeddings & FAISS index
output/
 â”œ faiss_index.bin
 â”œ metadata.pkl
 â”œ confusion_matrix.png
 â”œ classification_report.txt

ğŸ–¥ Run Streamlit Application
streamlit run streamlit_ui.py
User can now:

âœ” Upload audio
âœ” Enter text prompts e.g. "play drums"
âœ” Retrieve top-K relevant samples
ğŸ“Š Evaluation

Confusion matrix stored at:
output/confusion_matrix.png

Example screenshot ğŸ‘‡
(Insert your generated CM image here)

Also saved:

classification_report.txt

ğŸ” Zero-Shot Prompt Examples

| Prompt                   | Expected Retrieval |
| ------------------------ | ------------------ |
| â€œdrum beatsâ€             | Drums samples      |
| â€œbeautiful piano chordsâ€ | Keys samples       |

ğŸ“Œ Directory Structure
.
â”œ Data/
â”œ output/
â”œ metadata_simple.csv
â”œ embedding_index.py
â”œ streamlit_ui.py
â”œ zs_clap.py
â”œ requirements.txt
â”” README.md

ğŸ“Œ Future Enhancements

Add more instrument classes ğŸ» ğŸ· ğŸ¸

Apply quantization for large FAISS DB

Deploy Streamlit to cloud (Render / Hugging Face Spaces)

ğŸ Conclusion

This project demonstrates:

âœ” Multimodal learning
âœ” FAISS-based fast retrieval
âœ” Prompt-based semantic search
âœ” Zero-shot classification capability of CLAP